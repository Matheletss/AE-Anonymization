{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4923feda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\python310\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\python310\\lib\\site-packages (1.14.0)\n",
      "Requirement already satisfied: matplotlib in c:\\python310\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\python310\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\python310\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\python310\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\python310\\lib\\site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python310\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\python310\\lib\\site-packages (from matplotlib) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\python310\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python310\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pillow>=8 in c:\\python310\\lib\\site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\python310\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\python310\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\python310\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\python310\\lib\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.30.0)\n",
      "Requirement already satisfied: setuptools in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (63.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\python310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\python310\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\python310\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\python310\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\python310\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\python310\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\python310\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\python310\\lib\\site-packages (from packaging->tensorflow-intel==2.17.0->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\python310\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\python310\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\python310\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\python310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn scipy matplotlib\n",
    "!pip install tensorflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.inspection import permutation_importance\n",
    "from numpy import mean, max, prod, array, hstack\n",
    "from numpy.random import choice\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "feba6787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_autoencoder(dim_input, dim_layer_1, dim_layer_2):\n",
    "    \n",
    "    input_layer = Input(shape=(dim_input,))\n",
    "    x = Activation(\"relu\")(input_layer)\n",
    "    x = Dense(dim_layer_1)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    bottleneck_layer = Dense(dim_layer_2)(x)\n",
    "    x = Activation(\"relu\")(bottleneck_layer)\n",
    "    x = Dense(dim_layer_1)(x)\n",
    "    x = Activation(\"relu\")(x)    \n",
    "    output_layer = Dense(dim_input, activation='linear')(x)\n",
    "    \n",
    "    encoder = Model(input_layer, bottleneck_layer)\n",
    "    autoencoder = Model(input_layer, output_layer)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    print(f\"Autoencoder Input Dim: {dim_input}\")\n",
    "    print(f\"Autoencoder Output Dim: {dim_layer_1}\")\n",
    "    print(f\"Autoencoder Bottleneck Dim: {dim_layer_2}\")\n",
    "    \n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "73f0aad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['max', 'min', 'mean', 'sd', 'rms', 'skewness', 'kurtosis', 'crest',\n",
      "       'form', 'fault'],\n",
      "      dtype='object')\n",
      "       max      min      mean        sd       rms  skewness  kurtosis   \n",
      "0  0.35986 -0.41890  0.017840  0.122746  0.124006 -0.118571 -0.042219  \\\n",
      "1  0.46772 -0.36111  0.022255  0.132488  0.134312  0.174699 -0.081548   \n",
      "2  0.46855 -0.43809  0.020470  0.149651  0.151008  0.040339 -0.274069   \n",
      "3  0.58475 -0.54303  0.020960  0.157067  0.158422 -0.023266  0.134692   \n",
      "4  0.44685 -0.57891  0.022167  0.138189  0.139922 -0.081534  0.402783   \n",
      "\n",
      "      crest      form       fault  \n",
      "0  2.901946  6.950855  Ball_007_1  \n",
      "1  3.482334  6.035202  Ball_007_1  \n",
      "2  3.102819  7.376926  Ball_007_1  \n",
      "3  3.691097  7.558387  Ball_007_1  \n",
      "4  3.193561  6.312085  Ball_007_1  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"D:/Thoth project/datasets/CWRU bearing/CWRU_bearing.csv\", sep=\",\")\n",
    "df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "print(df.columns)\n",
    "print(df.head())\n",
    "feature_columns = ['max','min','mean','sd','rms','skewness','kurtosis','crest','form']\n",
    "target_column = 'fault'\n",
    "X = df[feature_columns]\n",
    "y = df[target_column]\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X = min_max_scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e6f94d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         ORIGINAL\n",
      "fit_time                 0.732199\n",
      "score_time               0.150745\n",
      "test_balanced_accuracy   0.793478\n",
      "train_balanced_accuracy  0.797053\n",
      "test_f1_weighted         0.746635\n",
      "train_f1_weighted        0.752148\n",
      "test_roc_auc                  NaN\n",
      "train_roc_auc                 NaN\n",
      "test_average_precision   0.807521\n",
      "train_average_precision  0.806786\n"
     ]
    }
   ],
   "source": [
    "#Baseline performance on original data\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=500, \n",
    "    max_depth=2, \n",
    "    n_jobs=8, \n",
    "    random_state=42\n",
    ")\n",
    "dict_performance = cross_validate(\n",
    "    estimator=rf, \n",
    "    X=X, y=y, \n",
    "    cv=10, \n",
    "    n_jobs=4,\n",
    "    return_train_score=True,\n",
    "    scoring=[\n",
    "        \"balanced_accuracy\", \n",
    "        \"f1_weighted\", \n",
    "        \"roc_auc\", \n",
    "        \"average_precision\"\n",
    "    ]\n",
    ")\n",
    "df_performance = pd.DataFrame(\n",
    "    {\"ORIGINAL\": [mean(dict_performance[k]) \\\n",
    "                  for k in dict_performance.keys()]}, \n",
    "    index=dict_performance.keys()\n",
    ")\n",
    "print(df_performance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa38755b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting the importance of the features in order to understand which one affects the target variable.\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=500, \n",
    "    max_depth=2, \n",
    "    n_jobs=8, \n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X, y)\n",
    "fi = permutation_importance(\n",
    "    estimator=rf, \n",
    "    X=X, \n",
    "    y=y, \n",
    "    n_repeats=10, \n",
    "    n_jobs=8, \n",
    "    random_state=42\n",
    ").importances_mean\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "sorted_idx = fi.argsort()[::-1]\n",
    "sorted_features = [feature_columns[i] for i in sorted_idx]\n",
    "plt.barh(\n",
    "    y=range(len(feature_columns)), \n",
    "    width=fi[sorted_idx], \n",
    "    alpha=0.9\n",
    ")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.yticks(\n",
    "    range(len(feature_columns)), \n",
    "    sorted_features\n",
    ")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.title(\"Original features importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "44f8d9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder Input Dim: 9\n",
      "Autoencoder Output Dim: 6\n",
      "Autoencoder Bottleneck Dim: 4\n"
     ]
    }
   ],
   "source": [
    "#building autoencoder\n",
    "dim_layer_input = X.shape[1]\n",
    "dim_layer_1 = max((int(3*dim_layer_input/4), 1))\n",
    "dim_layer_2 = max((int(dim_layer_input/2), 1))\n",
    "autoencoder, encoder = build_autoencoder(\n",
    "    dim_input=dim_layer_input,\n",
    "    dim_layer_1=dim_layer_1,\n",
    "    dim_layer_2=dim_layer_2,   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0a54e32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 692ms/step - loss: 0.1404\n",
      "Epoch 1: val_loss improved from inf to 0.12537, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.1373 - val_loss: 0.1254\n",
      "Epoch 2/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1239\n",
      "Epoch 2: val_loss improved from 0.12537 to 0.11219, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1222 - val_loss: 0.1122\n",
      "Epoch 3/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1096\n",
      "Epoch 3: val_loss improved from 0.11219 to 0.10106, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1089 - val_loss: 0.1011\n",
      "Epoch 4/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1005\n",
      "Epoch 4: val_loss improved from 0.10106 to 0.09159, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0987 - val_loss: 0.0916\n",
      "Epoch 5/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0925\n",
      "Epoch 5: val_loss improved from 0.09159 to 0.08343, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0899 - val_loss: 0.0834\n",
      "Epoch 6/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0824\n",
      "Epoch 6: val_loss improved from 0.08343 to 0.07627, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0816 - val_loss: 0.0763\n",
      "Epoch 7/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0762\n",
      "Epoch 7: val_loss improved from 0.07627 to 0.06998, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0744 - val_loss: 0.0700\n",
      "Epoch 8/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0683\n",
      "Epoch 8: val_loss improved from 0.06998 to 0.06446, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0682 - val_loss: 0.0645\n",
      "Epoch 9/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0640\n",
      "Epoch 9: val_loss improved from 0.06446 to 0.05965, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0627 - val_loss: 0.0597\n",
      "Epoch 10/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0578\n",
      "Epoch 10: val_loss improved from 0.05965 to 0.05544, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0580 - val_loss: 0.0554\n",
      "Epoch 11/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0546\n",
      "Epoch 11: val_loss improved from 0.05544 to 0.05177, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0544 - val_loss: 0.0518\n",
      "Epoch 12/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0513\n",
      "Epoch 12: val_loss improved from 0.05177 to 0.04858, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0508 - val_loss: 0.0486\n",
      "Epoch 13/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0476\n",
      "Epoch 13: val_loss improved from 0.04858 to 0.04579, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0474 - val_loss: 0.0458\n",
      "Epoch 14/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0458\n",
      "Epoch 14: val_loss improved from 0.04579 to 0.04337, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0450 - val_loss: 0.0434\n",
      "Epoch 15/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0440\n",
      "Epoch 15: val_loss improved from 0.04337 to 0.04120, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0426 - val_loss: 0.0412\n",
      "Epoch 16/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0414\n",
      "Epoch 16: val_loss improved from 0.04120 to 0.03923, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0405 - val_loss: 0.0392\n",
      "Epoch 17/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0387\n",
      "Epoch 17: val_loss improved from 0.03923 to 0.03743, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0386 - val_loss: 0.0374\n",
      "Epoch 18/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0371\n",
      "Epoch 18: val_loss improved from 0.03743 to 0.03574, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0368 - val_loss: 0.0357\n",
      "Epoch 19/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0330\n",
      "Epoch 19: val_loss improved from 0.03574 to 0.03414, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0346 - val_loss: 0.0341\n",
      "Epoch 20/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0348\n",
      "Epoch 20: val_loss improved from 0.03414 to 0.03254, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0339 - val_loss: 0.0325\n",
      "Epoch 21/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0338\n",
      "Epoch 21: val_loss improved from 0.03254 to 0.03094, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0322 - val_loss: 0.0309\n",
      "Epoch 22/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0290\n",
      "Epoch 22: val_loss improved from 0.03094 to 0.02935, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0302 - val_loss: 0.0293\n",
      "Epoch 23/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0283\n",
      "Epoch 23: val_loss improved from 0.02935 to 0.02778, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0287 - val_loss: 0.0278\n",
      "Epoch 24/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0275\n",
      "Epoch 24: val_loss improved from 0.02778 to 0.02623, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0273 - val_loss: 0.0262\n",
      "Epoch 25/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0256\n",
      "Epoch 25: val_loss improved from 0.02623 to 0.02472, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0259 - val_loss: 0.0247\n",
      "Epoch 26/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0253\n",
      "Epoch 26: val_loss improved from 0.02472 to 0.02326, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0247 - val_loss: 0.0233\n",
      "Epoch 27/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0227\n",
      "Epoch 27: val_loss improved from 0.02326 to 0.02189, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0229 - val_loss: 0.0219\n",
      "Epoch 28/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0230\n",
      "Epoch 28: val_loss improved from 0.02189 to 0.02059, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0220 - val_loss: 0.0206\n",
      "Epoch 29/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0218\n",
      "Epoch 29: val_loss improved from 0.02059 to 0.01939, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0210 - val_loss: 0.0194\n",
      "Epoch 30/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0208\n",
      "Epoch 30: val_loss improved from 0.01939 to 0.01828, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0199 - val_loss: 0.0183\n",
      "Epoch 31/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0183\n",
      "Epoch 31: val_loss improved from 0.01828 to 0.01726, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0181 - val_loss: 0.0173\n",
      "Epoch 32/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0163\n",
      "Epoch 32: val_loss improved from 0.01726 to 0.01634, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0173 - val_loss: 0.0163\n",
      "Epoch 33/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0162\n",
      "Epoch 33: val_loss improved from 0.01634 to 0.01550, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0165 - val_loss: 0.0155\n",
      "Epoch 34/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0155\n",
      "Epoch 34: val_loss improved from 0.01550 to 0.01475, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0154 - val_loss: 0.0147\n",
      "Epoch 35/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0132\n",
      "Epoch 35: val_loss improved from 0.01475 to 0.01408, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0147 - val_loss: 0.0141\n",
      "Epoch 36/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0163\n",
      "Epoch 36: val_loss improved from 0.01408 to 0.01348, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0146 - val_loss: 0.0135\n",
      "Epoch 37/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0133\n",
      "Epoch 37: val_loss improved from 0.01348 to 0.01293, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0136 - val_loss: 0.0129\n",
      "Epoch 38/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0139\n",
      "Epoch 38: val_loss improved from 0.01293 to 0.01244, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0131 - val_loss: 0.0124\n",
      "Epoch 39/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0126\n",
      "Epoch 39: val_loss improved from 0.01244 to 0.01199, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0127 - val_loss: 0.0120\n",
      "Epoch 40/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0116\n",
      "Epoch 40: val_loss improved from 0.01199 to 0.01160, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0119 - val_loss: 0.0116\n",
      "Epoch 41/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0122\n",
      "Epoch 41: val_loss improved from 0.01160 to 0.01124, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0119 - val_loss: 0.0112\n",
      "Epoch 42/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0107\n",
      "Epoch 42: val_loss improved from 0.01124 to 0.01092, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0112 - val_loss: 0.0109\n",
      "Epoch 43/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0126\n",
      "Epoch 43: val_loss improved from 0.01092 to 0.01064, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0113 - val_loss: 0.0106\n",
      "Epoch 44/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0106\n",
      "Epoch 44: val_loss improved from 0.01064 to 0.01038, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0108 - val_loss: 0.0104\n",
      "Epoch 45/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0111\n",
      "Epoch 45: val_loss improved from 0.01038 to 0.01014, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0107 - val_loss: 0.0101\n",
      "Epoch 46/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0091\n",
      "Epoch 46: val_loss improved from 0.01014 to 0.00993, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0099 - val_loss: 0.0099\n",
      "Epoch 47/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0091\n",
      "Epoch 47: val_loss improved from 0.00993 to 0.00972, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0096 - val_loss: 0.0097\n",
      "Epoch 48/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0094\n",
      "Epoch 48: val_loss improved from 0.00972 to 0.00955, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0097 - val_loss: 0.0095\n",
      "Epoch 49/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0092\n",
      "Epoch 49: val_loss improved from 0.00955 to 0.00937, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0094 - val_loss: 0.0094\n",
      "Epoch 50/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0095\n",
      "Epoch 50: val_loss improved from 0.00937 to 0.00922, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0097 - val_loss: 0.0092\n",
      "Epoch 51/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0099\n",
      "Epoch 51: val_loss improved from 0.00922 to 0.00907, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0096 - val_loss: 0.0091\n",
      "Epoch 52/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0081\n",
      "Epoch 52: val_loss improved from 0.00907 to 0.00894, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0091 - val_loss: 0.0089\n",
      "Epoch 53/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0082\n",
      "Epoch 53: val_loss improved from 0.00894 to 0.00882, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0089 - val_loss: 0.0088\n",
      "Epoch 54/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0087\n",
      "Epoch 54: val_loss improved from 0.00882 to 0.00871, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0089 - val_loss: 0.0087\n",
      "Epoch 55/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0078\n",
      "Epoch 55: val_loss improved from 0.00871 to 0.00860, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0086 - val_loss: 0.0086\n",
      "Epoch 56/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0086\n",
      "Epoch 56: val_loss improved from 0.00860 to 0.00851, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0088 - val_loss: 0.0085\n",
      "Epoch 57/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0078\n",
      "Epoch 57: val_loss improved from 0.00851 to 0.00842, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0085 - val_loss: 0.0084\n",
      "Epoch 58/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0084\n",
      "Epoch 58: val_loss improved from 0.00842 to 0.00834, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0084 - val_loss: 0.0083\n",
      "Epoch 59/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0083\n",
      "Epoch 59: val_loss improved from 0.00834 to 0.00827, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0084 - val_loss: 0.0083\n",
      "Epoch 60/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0092\n",
      "Epoch 60: val_loss improved from 0.00827 to 0.00820, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0085 - val_loss: 0.0082\n",
      "Epoch 61/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0098\n",
      "Epoch 61: val_loss improved from 0.00820 to 0.00814, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0087 - val_loss: 0.0081\n",
      "Epoch 62/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0089\n",
      "Epoch 62: val_loss improved from 0.00814 to 0.00807, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0083 - val_loss: 0.0081\n",
      "Epoch 63/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0078\n",
      "Epoch 63: val_loss improved from 0.00807 to 0.00802, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0080 - val_loss: 0.0080\n",
      "Epoch 64/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0076\n",
      "Epoch 64: val_loss improved from 0.00802 to 0.00797, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0080 - val_loss: 0.0080\n",
      "Epoch 65/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0083\n",
      "Epoch 65: val_loss improved from 0.00797 to 0.00792, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0079 - val_loss: 0.0079\n",
      "Epoch 66/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0082\n",
      "Epoch 66: val_loss improved from 0.00792 to 0.00788, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0079 - val_loss: 0.0079\n",
      "Epoch 67/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0082\n",
      "Epoch 67: val_loss improved from 0.00788 to 0.00785, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0082 - val_loss: 0.0078\n",
      "Epoch 68/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0082\n",
      "Epoch 68: val_loss improved from 0.00785 to 0.00779, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0078 - val_loss: 0.0078\n",
      "Epoch 69/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0070\n",
      "Epoch 69: val_loss improved from 0.00779 to 0.00775, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0077 - val_loss: 0.0077\n",
      "Epoch 70/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0075\n",
      "Epoch 70: val_loss improved from 0.00775 to 0.00771, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0077 - val_loss: 0.0077\n",
      "Epoch 71/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0073\n",
      "Epoch 71: val_loss improved from 0.00771 to 0.00767, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0076 - val_loss: 0.0077\n",
      "Epoch 72/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0095\n",
      "Epoch 72: val_loss improved from 0.00767 to 0.00763, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0082 - val_loss: 0.0076\n",
      "Epoch 73/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0084\n",
      "Epoch 73: val_loss improved from 0.00763 to 0.00759, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0079 - val_loss: 0.0076\n",
      "Epoch 74/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0074\n",
      "Epoch 74: val_loss improved from 0.00759 to 0.00755, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0075 - val_loss: 0.0076\n",
      "Epoch 75/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0073\n",
      "Epoch 75: val_loss improved from 0.00755 to 0.00752, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0075 - val_loss: 0.0075\n",
      "Epoch 76/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0083\n",
      "Epoch 76: val_loss improved from 0.00752 to 0.00749, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0077 - val_loss: 0.0075\n",
      "Epoch 77/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0076\n",
      "Epoch 77: val_loss improved from 0.00749 to 0.00746, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0076 - val_loss: 0.0075\n",
      "Epoch 78/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0083\n",
      "Epoch 78: val_loss improved from 0.00746 to 0.00742, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0076 - val_loss: 0.0074\n",
      "Epoch 79/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0083\n",
      "Epoch 79: val_loss improved from 0.00742 to 0.00739, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0077 - val_loss: 0.0074\n",
      "Epoch 80/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0070\n",
      "Epoch 80: val_loss improved from 0.00739 to 0.00736, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0074 - val_loss: 0.0074\n",
      "Epoch 81/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0077\n",
      "Epoch 81: val_loss improved from 0.00736 to 0.00733, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0077 - val_loss: 0.0073\n",
      "Epoch 82/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0067\n",
      "Epoch 82: val_loss improved from 0.00733 to 0.00729, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0071 - val_loss: 0.0073\n",
      "Epoch 83/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0074\n",
      "Epoch 83: val_loss improved from 0.00729 to 0.00726, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0072 - val_loss: 0.0073\n",
      "Epoch 84/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0075\n",
      "Epoch 84: val_loss improved from 0.00726 to 0.00724, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0076 - val_loss: 0.0072\n",
      "Epoch 85/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0069\n",
      "Epoch 85: val_loss improved from 0.00724 to 0.00721, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0073 - val_loss: 0.0072\n",
      "Epoch 86/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0070\n",
      "Epoch 86: val_loss improved from 0.00721 to 0.00717, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0073 - val_loss: 0.0072\n",
      "Epoch 87/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0082\n",
      "Epoch 87: val_loss improved from 0.00717 to 0.00714, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0073 - val_loss: 0.0071\n",
      "Epoch 88/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0072\n",
      "Epoch 88: val_loss improved from 0.00714 to 0.00710, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0072 - val_loss: 0.0071\n",
      "Epoch 89/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0073\n",
      "Epoch 89: val_loss improved from 0.00710 to 0.00707, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0073 - val_loss: 0.0071\n",
      "Epoch 90/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0074\n",
      "Epoch 90: val_loss improved from 0.00707 to 0.00703, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0071 - val_loss: 0.0070\n",
      "Epoch 91/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0069\n",
      "Epoch 91: val_loss improved from 0.00703 to 0.00700, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 92/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0067\n",
      "Epoch 92: val_loss improved from 0.00700 to 0.00698, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 93/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0067\n",
      "Epoch 93: val_loss improved from 0.00698 to 0.00695, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0071 - val_loss: 0.0069\n",
      "Epoch 94/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0079\n",
      "Epoch 94: val_loss improved from 0.00695 to 0.00690, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0073 - val_loss: 0.0069\n",
      "Epoch 95/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0080\n",
      "Epoch 95: val_loss improved from 0.00690 to 0.00687, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0071 - val_loss: 0.0069\n",
      "Epoch 96/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0070\n",
      "Epoch 96: val_loss improved from 0.00687 to 0.00685, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 97/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0064\n",
      "Epoch 97: val_loss improved from 0.00685 to 0.00682, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0066 - val_loss: 0.0068\n",
      "Epoch 98/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0072\n",
      "Epoch 98: val_loss improved from 0.00682 to 0.00682, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0070 - val_loss: 0.0068\n",
      "Epoch 99/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0067\n",
      "Epoch 99: val_loss improved from 0.00682 to 0.00678, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0067 - val_loss: 0.0068\n",
      "Epoch 100/100\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0065\n",
      "Epoch 100: val_loss improved from 0.00678 to 0.00675, saving model to best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0068 - val_loss: 0.0067\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step\n",
      "Encoded Train Shape: (1840, 4)\n",
      "Encoded Test Shape: (460, 4)\n",
      "Anonymized Training Data:\n",
      "   encoded_1  encoded_2  encoded_3  encoded_4\n",
      "0  -0.584199   0.033050   1.272598   0.261227\n",
      "1  -0.471851  -0.018225   1.095231   0.075776\n",
      "2  -0.891192   0.442979   1.617889   0.965836\n",
      "3  -0.442310  -0.010435   1.078979   0.049683\n",
      "4  -1.077990   0.849190   1.900486   1.144599\n",
      "\n",
      "Anonymized Testing Data:\n",
      "   encoded_1  encoded_2  encoded_3  encoded_4\n",
      "0  -0.694791   0.332177   1.590471   0.320205\n",
      "1  -1.298256   1.050780   2.227156   1.591293\n",
      "2  -0.582778   0.131358   1.479566   0.041588\n",
      "3  -1.564233   1.316764   2.461036   1.648735\n",
      "4  -0.556305  -0.011771   1.177263   0.170638\n"
     ]
    }
   ],
   "source": [
    "#Training the model \n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=20, verbose=1),\n",
    "    ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "autoencoder.fit(\n",
    "    X_train, X_train,\n",
    "    epochs=100,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, X_test),\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Transform data\n",
    "X_train_encoded = encoder.predict(X_train)\n",
    "X_test_encoded = encoder.predict(X_test)\n",
    "df_train_encoded = pd.DataFrame(X_train_encoded, columns=[f\"encoded_{i+1}\" for i in range(X_train_encoded.shape[1])])\n",
    "df_test_encoded = pd.DataFrame(X_test_encoded, columns=[f\"encoded_{i+1}\" for i in range(X_test_encoded.shape[1])])\n",
    "print(f\"Encoded Train Shape: {X_train_encoded.shape}\")\n",
    "print(f\"Encoded Test Shape: {X_test_encoded.shape}\")\n",
    "\n",
    "print(\"Anonymized Training Data:\")\n",
    "print(df_train_encoded.head())\n",
    "\n",
    "print(\"\\nAnonymized Testing Data:\")\n",
    "print(df_test_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1c7cf3e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         ORIGINAL   ENCODED\n",
      "fit_time                 1.035198  0.963198\n",
      "score_time               0.171200  0.181901\n",
      "test_balanced_accuracy   0.938283  0.901054\n",
      "train_balanced_accuracy  0.947683  0.924699\n",
      "test_f1_weighted         0.939209  0.899925\n",
      "train_f1_weighted        0.948957  0.924395\n",
      "test_roc_auc                  NaN       NaN\n",
      "train_roc_auc                 NaN       NaN\n",
      "test_average_precision   0.969928  0.946033\n",
      "train_average_precision  0.978222  0.962059\n"
     ]
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=500, max_depth=5, n_jobs=8, random_state=42)\n",
    "\n",
    "dict_performance_original = cross_validate(\n",
    "    estimator=rf_classifier,\n",
    "    X=X_train, y=y_train,\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    return_train_score=True,\n",
    "    scoring=[\"balanced_accuracy\", \"f1_weighted\", \"roc_auc\", \"average_precision\"]\n",
    ")\n",
    "df_performance = pd.DataFrame(\n",
    "    {\"ORIGINAL\": [mean(dict_performance_original[k]) for k in dict_performance_original.keys()]},\n",
    "    index=dict_performance_original.keys()\n",
    ")\n",
    "\n",
    "dict_performance_encoded = cross_validate(\n",
    "    estimator=rf_classifier,\n",
    "    X=X_train_encoded, y=y_train,\n",
    "    cv=10,\n",
    "    n_jobs=4,\n",
    "    return_train_score=True,\n",
    "    scoring=[\"balanced_accuracy\", \"f1_weighted\", \"roc_auc\", \"average_precision\"]\n",
    ")\n",
    "df_performance[\"ENCODED\"] = [mean(dict_performance_encoded[k]) for k in dict_performance_encoded.keys()]\n",
    "\n",
    "print(df_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "303553b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 0.0069\n",
      "Reconstruction Loss: 0.006746251601725817\n"
     ]
    }
   ],
   "source": [
    "reconstruction_loss = autoencoder.evaluate(X_test, X_test)\n",
    "print(f'Reconstruction Loss: {reconstruction_loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
